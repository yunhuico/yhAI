[TOC]

# metrics-collector

# Summary

metrics-collector is a middle server between cAdvisors and Prometheus.
It retrieves metrics from multiple cAdvisor instances asynchronously. When all
metrics are fetched, it will do the following

1. filter metrics lines by index and keep ones containing `usage`
  (actually this is done when parsing HTTP respose);
2. unmarshal metrics lines to struct and separate lines by `app_id` ;
3. calculate the average CPU, memory percentage of containers;
4. combine the results as metrics.

The averaged result can indicate if the containers are overloaded (or idle).
The word 'containers' here means multiple instances of a Docker container,
it's also called 'component' in DC/OS, and 'application' in Marathon.

metrics-collector also fetched CPU and Memory usage of the **host** machines,
the stats data are retrieved from cAdvisor API `/api/<version>/containers`. They
are used to monitor host machines.

metrics-collector provides an API just like the cAdvisor `/metrics` path
to expose the calculated statistics. Prometheus will call this API and
fetch the metrics.

metrics-collector is supposed to be deployed in Linker DC/OS user-cluster,
together with cAdvisor, Prometheus and alert-manager.

This project is inspired by [cadvprom][1].

# Prerequisites

* git
* make
* Docker 17.05.0-ce+

# Build
Get dependencies with govendor.

```sh
make install-vendor
make get-dep
```

Build docker image,

```sh
sudo make
```

# Run

# Environment Variables

This is a table of (optional) environment variables in metrics-collector container.

|Env Key|Type|Example Value|Default|Meaning|
|:--|:--:|:--:|:--:|:--|
|DAEMON_MODE|string|"onrequest" / "polling"|"onrequest"|Policy for daemon on how to retrieve metrics|
|CADVISORS|string array|"192.168.1.100:10000;192.168.1.101:10000"|""|semicolon separated cAdvisor addresses|
|POLLING_SEC|int|5|5|The interval (seconds) each time the daemon fetches metrics in mode 'polling'|
|CADVISOR_TIMEOUT|int|5000|5000|The HTTP i/o timeout (milliseconds) when calling cAdvisor `/metrics` API|
|ADDR_UPDATE_SEC|int|300|300|The interval (seconds) each time the daemon refresh cAdvisor addresses from mesos `/slaves` API|
|MESOS_ENDPOINT|string|192.168.1.100:5050|master.mesos/mesos|Mesos leader IP or domain name|
|CADVISOR_PORT|int|10000|10000|Port of cAdvisor|
|ENABLE_HOST_MONITOR|bool|true|false|Set true to enable host monitor|


# Example 1

mode "polling", auto address updating not enabled

```sh
# set mode
sudo export DAEMON_MODE=polling

# set metrics source
sudo export CADVISORS="192.168.1.100:10000;192.168.1.101:10000"

# (optional) change polling second
sudo export POLLING_SEC=5

# (optional) change request timeout, default 5000, in millisecond
sudo export CADVISOR_TIMEOUT=2000

# enable host monitor
sudo export ENABLE_HOST_MONITOR=true
# start container
sudo make run
```

The daemon will retrieve metrics every 5s from `192.168.1.100:10000` and `192.168.1.101:10000`

## Example 2
mode "onrequest", with auto address updating

```sh
# (optional) set mode
sudo export DAEMON_MODE=onrequest

# change mesos endpoint on your development environment
sudo export MESOS_ENDPOINT="192.168.1.100:5050"

# enable updater
sudo export ENABLE_UPDATER=true

# change updater interval
sudo export ADDR_UPDATE_SEC=30

# (optional) change cadvisor port
sudo export CADVISOR_PORT=10000

# enable host monitor
sudo export ENABLE_HOST_MONITOR=true
# start container
make run
```

The daemon will call mesos API to fetch cAdvisor addresses every 30s, and will retrieve metrics
from those cAdvisors on every request calling `GET /metrics` of this metrics-collector.

# APIs

## Ping

`GET /`

Example:

**Response**

```
Linker metrics-collector server
```

## Metrics

`GET /metrics`

Example:

**Response**
```
# Generated by metrics-collector
# TYPE container_cpu_usage_high_result gauge
# TYPE container_cpu_usage_low_result gauge
# TYPE container_memory_usage_high_result gauge
# TYPE container_memory_usage_low_result gauge
container_cpu_usage_high_result{alert="true",alert_name="HighCpuAlert",app_container_id="",app_id="/stress/stress",group_id="/stress",image="zyfdedh/stress",repair_template_id="stress",service_group_id="",service_group_instance_id="",service_order_id=""} 1.000000e+00
container_cpu_usage_low_result{alert="true",alert_name="LowCpuAlert",app_container_id="",app_id="/stress/stress",group_id="/stress",image="zyfdedh/stress",repair_template_id="stress",service_group_id="",service_group_instance_id="",service_order_id=""} 1.000000e+00
container_memory_usage_high_result{alert="true",alert_name="HighMemoryAlert",app_container_id="",app_id="/stress/stress",group_id="/stress",image="zyfdedh/stress",repair_template_id="stress",service_group_id="",service_group_instance_id="",service_order_id=""} 2.441406e-02
container_memory_usage_low_result{alert="true",alert_name="LowMemoryAlert",app_container_id="",app_id="/stress/stress",group_id="/stress",image="zyfdedh/stress",repair_template_id="stress",service_group_id="",service_group_instance_id="",service_order_id=""} 7.324219e-02
# TYPE host_cpu_usage gauge
# TYPE host_memory_usage gauge
host_cpu_usage{alert="true",alert_name="HostHighCpuAlert",host_ip="192.168.10.214"} 1.236800e+00
host_cpu_usage{alert="true",alert_name="HostLowCpuAlert",host_ip="192.168.10.214"} 1.236800e+00
host_memory_usage{alert="true",alert_name="HostHighMemoryAlert",host_ip="192.168.10.214"} 8.760107e+01
host_memory_usage{alert="true",alert_name="HostLowMemoryAlert",host_ip="192.168.10.214"} 8.760107e+01
host_cpu_usage{alert="true",alert_name="HostHighCpuAlert",host_ip="192.168.10.213"} 2.544874e+01
host_cpu_usage{alert="true",alert_name="HostLowCpuAlert",host_ip="192.168.10.213"} 2.544874e+01
host_memory_usage{alert="true",alert_name="HostHighMemoryAlert",host_ip="192.168.10.213"} 8.926467e+01
host_memory_usage{alert="true",alert_name="HostLowMemoryAlert",host_ip="192.168.10.213"} 8.926467e+01
```

## PProf

This API is used for debugging and performance optimization.

It's not enabled by default, to enable, add argument like `./metrics-collector -pprof`.

And then visit `/debug/pprof` in web browser.

Example:

**Request**

`GET /debug/pprof/goroutine?debug=1`
`GET /debug/pprof/goroutine?debug=2`
`GET /debug/pprof/heap?debug=1`
`GET /debug/pprof/threadcreate?debug=1`
`GET /debug/pprof/block?debug=1`
`GET /debug/pprof/mutex?debug=1`

[1]: https://bitbucket.org/linkernetworks/cadvprom
